{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYs3wxL43tVOjhfNXRqAVS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raihanewubd/24FALL-AI/blob/main/AI_Lab_05a_Pretrained_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tzn_HArq0mq",
        "outputId": "0a9b1d1f-0e65-4ed1-e92b-9d7068b12f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rifat963/pumpkin?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675M/675M [00:10<00:00, 70.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rifat963/pumpkin/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rifat963/pumpkin\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "src = '/root/.cache/kagglehub/datasets/rifat963/pumpkin/versions/2'\n",
        "dst = '/content'\n",
        "\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3JpBLhh8rnO1",
        "outputId": "78db7abe-c48e-49c3-e739-80f94a478487"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "XBoJFywLq-N1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxfqIbeOrBrF",
        "outputId": "627875ca-a65c-4a8b-fad9-3fe7863ba26e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transforms for training and validation"
      ],
      "metadata": {
        "id": "2nPaYEk4rJwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "KvCJGUAQrLAj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/Augmented/Augmented'  # Update this with your local path\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
        "                                              shuffle=True, num_workers=4)\n",
        "               for x in ['train', 'val']}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N1VKUsMr_Ti",
        "outputId": "e0bcdde6-7ef7-4508-a6de-426854f9ab5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of pretrained models to experiment with\n",
        "pretrained_models = {\n",
        "    'resnet18': models.resnet18(pretrained=True),\n",
        "    #'resnet50': models.resnet50(pretrained=True),\n",
        "    #'efficientnet_b0': models.efficientnet_b0(pretrained=True)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER1mm6lksPfa",
        "outputId": "8d585347-7fd7-4c2a-a071-6d88ee2d191a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(class_names)\n",
        "print(f\"Number of classes:{num_classes}\")\n",
        "num_epochs = 2  # Adjust as needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXfKhaNWsSQq",
        "outputId": "fa6497e0-2b55-4295-dd05-c39ee8371a34"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes:5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, model in pretrained_models.items():\n",
        "    # Modify the final layer according to the architecture\n",
        "    if name.startswith('resnet'):\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif 'efficientnet' in name:\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    print(f\"\\nTraining model: {name}\")\n",
        "\n",
        "    # Basic training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIfBTXqbscXK",
        "outputId": "a41efa70-488a-43b5-8d70-cb33811c6ea5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model: resnet18\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7747 Acc: 0.7117\n",
            "Val Loss: 0.5651 Acc: 0.7995\n",
            "Epoch 2/10\n",
            "Train Loss: 0.4806 Acc: 0.8264\n",
            "Val Loss: 0.5210 Acc: 0.8246\n",
            "Epoch 3/10\n",
            "Train Loss: 0.4075 Acc: 0.8490\n",
            "Val Loss: 0.5336 Acc: 0.8371\n",
            "Epoch 4/10\n",
            "Train Loss: 0.3737 Acc: 0.8643\n",
            "Val Loss: 0.5029 Acc: 0.8321\n",
            "Epoch 5/10\n",
            "Train Loss: 0.3487 Acc: 0.8705\n",
            "Val Loss: 0.5198 Acc: 0.8396\n",
            "Epoch 6/10\n",
            "Train Loss: 0.3380 Acc: 0.8721\n",
            "Val Loss: 0.5534 Acc: 0.8421\n",
            "Epoch 7/10\n",
            "Train Loss: 0.3011 Acc: 0.8881\n",
            "Val Loss: 0.5560 Acc: 0.8346\n",
            "Epoch 8/10\n",
            "Train Loss: 0.2967 Acc: 0.8998\n",
            "Val Loss: 0.5579 Acc: 0.8346\n",
            "Epoch 9/10\n",
            "Train Loss: 0.2663 Acc: 0.9029\n",
            "Val Loss: 0.5321 Acc: 0.8471\n",
            "Epoch 10/10\n",
            "Train Loss: 0.2604 Acc: 0.9026\n",
            "Val Loss: 0.5433 Acc: 0.8346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(model, model_name):\n",
        "    # Modify final layer according to the architecture\n",
        "    if model_name.startswith('resnet'):\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif 'efficientnet' in model_name:\n",
        "        num_ftrs = model.classifier[1].in_features\n",
        "        model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    print(f\"\\nTraining model: {model_name}\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # After training, evaluate on the validation set:\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    all_probs = []  # to store probabilities for ROC curve\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['val']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # Classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "\n",
        "    # ROC curve for each class (One-vs-Rest)\n",
        "    fpr = {}\n",
        "    tpr = {}\n",
        "    roc_auc = {}\n",
        "    # Binarize the output labels for multi-class ROC computation\n",
        "    labels_binarized = label_binarize(all_labels, classes=range(num_classes))\n",
        "\n",
        "    plt.figure()\n",
        "    for i in range(num_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(labels_binarized[:, i], all_probs[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        plt.plot(fpr[i], tpr[i],\n",
        "                 label=f'ROC curve for {class_names[i]} (area = {roc_auc[i]:0.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random chance\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves for {model_name}')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4-XFX1gKtun5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each pretrained model, train and evaluate\n",
        "for name, model in pretrained_models.items():\n",
        "    train_and_evaluate_model(model, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nEuJlhMtyTy",
        "outputId": "3ef744c4-820b-4f50-f7ac-f7a6858c3fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model: resnet18\n",
            "Epoch 1/2\n",
            "Train Loss: 0.4368 Acc: 0.8462\n",
            "Val Loss: 0.5368 Acc: 0.8396\n",
            "Epoch 2/2\n"
          ]
        }
      ]
    }
  ]
}